"""Graph builder for an intelligent RAG workflow using an LLM for decision logic"""

from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langgraph.checkpoint.memory import MemorySaver

# Assurez-vous que RAGState est un TypedDict et que les n≈ìuds retournent des dictionnaires
from src.state.rag_state import RAGState
from src.node.reactnode import RAGNodes
from src.config.config import Config



class GraphBuilder:
    """Builds and manages the intelligent LangGraph workflow using an LLM for decision-making"""
    
    def __init__(self, retriever, llm):
        """
        Initialise le constructeur du graph.
        
        Args:
            retriever: L'instance du retriever de documents.
            llm: L'instance du mod√®le de langage.
        """
        self.nodes = RAGNodes(retriever, llm)
        self.graph = None
        self.llm = llm
    
    def _decide_next_step(self, state: RAGState) -> str:
        """
        Fonction de d√©cision utilisant le LLM pour d√©terminer si les documents r√©cup√©r√©s sont suffisants.
        """
        print("---DECISION: √âvaluation de la pertinence des documents avec le LLM---")
        docs = state.retrieved_docs
        question = state.question
        
        # Si aucun document n'est trouv√©, il faut obligatoirement utiliser l'agent
        if not docs:
            print("---DECISION: Aucun document trouv√©. Passage √† l'agent de recherche.---")
            return "agent_search"
        
        # Formate le contenu des documents pour le LLM
        context = "\n\n---\n\n".join([doc.page_content for doc in docs])
        
        # Cr√©e un prompt pour demander au LLM d'√©valuer la pertinence
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", 
             "Vous √™tes un arbitre expert. Votre r√¥le est de d√©terminer si les documents fournis contiennent suffisamment d'informations pour r√©pondre de mani√®re compl√®te √† la question de l'utilisateur. "
             "R√©pondez uniquement par 'oui' si les documents sont suffisants, et 'non' s'ils ne le sont pas ou si des informations externes sont n√©cessaires."),
            ("user", 
             "Question: {question}\n\n"
             "Documents:\n{context}")
        ])
        
        # Cr√©e une cha√Æne et appelle le LLM
        chain = prompt_template | self.llm
        response = chain.invoke({"question": question, "context": context})
        
        decision = response.content.strip().lower()
        print(f"---DECISION: Le LLM a r√©pondu '{decision}'.---")
        
        if "oui" in decision:
            print("---DECISION: Documents jug√©s suffisants. Passage √† la g√©n√©ration directe.---")
            return "generate_direct"
        else:
            print("---DECISION: Documents jug√©s insuffisants. Passage √† l'agent de recherche.---")
            return "agent_search"

    def _generate_direct_answer(self, state: RAGState) -> dict:
        """
        G√©n√®re une r√©ponse directement √† partir des documents r√©cup√©r√©s.
        """
        print("---üìù N≈íUD: G√©n√©ration directe avec documents---")
        docs = state.retrieved_docs
        question = state.question
        
        context = "\n\n".join([doc.page_content for doc in docs])
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "Vous √™tes un assistant expert en RAG. R√©pondez √† la question en vous basant uniquement sur le contexte suivant:\n\n{context}"),
            ("user", "Question: {question}")
        ])
        
        chain = prompt_template | self.llm
        response = chain.invoke({"context": context, "question": question})
        
        # Les n≈ìuds doivent retourner un dictionnaire des champs mis √† jour
        return {"answer": response.content}
    
    def build(self):
        """
        Construit le graph RAG intelligent avec la logique de d√©cision du LLM.
        """
        builder = StateGraph(RAGState)
        
        # Ajout des n≈ìuds
        builder.add_node("retriever", self.nodes.retrieve_docs)
        builder.add_node("generate_direct", self._generate_direct_answer)
        builder.add_node("agent_search", self.nodes.generate_answer)
        
        # D√©finition du point d'entr√©e
        builder.set_entry_point("retriever")
        
        # Ajout des ar√™tes conditionnelles bas√©es sur la d√©cision du LLM
        builder.add_conditional_edges(
            "retriever",
            self._decide_next_step,
            {
                "generate_direct": "generate_direct",
                "agent_search": "agent_search"
            }
        )
        
        # Ajout des ar√™tes finales
        builder.add_edge("generate_direct", END)
        builder.add_edge("agent_search", END)
        memory = MemorySaver()
        self.graph = builder.compile(checkpointer=memory)
        
        # Sauvegarde de l'image du graph
        try:
            image_data = self.graph.get_graph().draw_mermaid_png()
            with open("intelligent_graph.png", "wb") as f:
                f.write(image_data)
            print("Image du graph sauvegard√©e dans 'intelligent_graph.png'")
        except Exception as e:
            print(f"Impossible de sauvegarder la visualisation du graph : {e}")
            
        return self.graph
    
    def run(self, question: str, thread_id: str) -> dict:
        """
        Ex√©cute le workflow RAG intelligent pour un fil de discussion sp√©cifique.
        """
        if self.graph is None:
            self.build()
        
        initial_state = {"question": question}
        config = Config.get_thread_config(thread_id)
        
        return self.graph.invoke(initial_state, config=config)